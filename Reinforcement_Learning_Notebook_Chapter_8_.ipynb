{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning Notebook Chapter 8-.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cing3000/Reinforcement-Learning-An-Introduction/blob/master/Reinforcement_Learning_Notebook_Chapter_8_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vdhMJ8-vX8p9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter 8: Planning and Learning with Tabular Methods\n",
        "\n",
        "Methods that require a model of the environment, such as dynamic programming and heuristic search, are called _model-based_ reinforcement learning methods.\n",
        "\n",
        "Methods that can be used without a model, such Monte Carlo and temporal-difference methods, are called _model-free_ reinforcement learning methods.\n",
        "\n",
        "Model-based methods rely on _planning_ as their primary component, while model-free methods primarily rely on _learning_."
      ]
    },
    {
      "metadata": {
        "id": "gGm4mjJ8pBVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.1 Models and Planning\n",
        "\n",
        "Distribution models: produce a description of all possibilities and their probabilities.\n",
        "Sample models: product just one of the possibilities, sampled according to the probabilities.\n",
        "\n",
        "We use the term _planning_ to refer to any computational process that takes a model as input and produces or imporoves a policy for interacting with the modeled environment.\n",
        "\n",
        "State-space planning: is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal.\n",
        "\n",
        "Plan-space planning: **?**\n",
        "\n",
        "This common structure can be diagrammed as follows:\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*_mm9H3rRvaTseXEim47saw.png\" width=\"800\"/>\n",
        "\n",
        "<font color='red'>The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment.</font>\n",
        "\n",
        "** Random-sample one-step tabular Q-planning **\n",
        "\n",
        "---\n",
        "Do forever:\n",
        "1. Select a state, $S\\in\\mathcal{S}$, and an action, $A \\in\\mathcal{A}(s)$, at random\n",
        "1. Send $S$, $A$ to a sample model, and obtain: a sample next reward, $R$, and a sample next state, $S'$\n",
        "1. Apply one-step tabular Q-learning to $S$, $A$, $R$, $S'$:\n",
        ">$Q(S,A)\\leftarrow Q(S,A)+\\alpha\\big[R+\\gamma \\max_aQ(S',a)-Q(S,A)\\big]$\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "MXM4SCP0EbRP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 8.2 Dyna: Integrating Plannning, Acting and Learning\n",
        "\n",
        "**Dyna-Q**: a simple architecture integrating the major functions needed in an on-line planning agent.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*XU8ikxAFcn942MueCXK1aw.png\" width=\"600\" />\n",
        "\n",
        "Dyna-Q includes all of the processes shown in Figure 8.1, all occurring continuously. The planning method is the random-sample one-step tabular Q-planning method. The direct RL method is one-step tabular Q-learning. The model-learning method is also table-based and assumes the environment is deterministic. After each transition $S_t,A_t \\rightarrow R_{t+1},S_{t+1}$, the model records in its table entry for $S_t, A_t$ the prediction that $R_{t+1}, S_{t+1}$ will deterministically follow."
      ]
    },
    {
      "metadata": {
        "id": "GdSFpJEHpArI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}